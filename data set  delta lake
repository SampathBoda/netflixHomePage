+-------+---+---------+
|   name|age|  country|
+-------+---+---------+
|   john| 25|      USA|
|   Mary| 31|       UK|
|  David| 42|Australia|
|  Emily| 28|      USA|
|Michael| 35|       UK|
+-------+---+---------+




















































































































































































































































!apt-get install openjdk-11-jdk-headless -qq > /dev/null

!pip install pyspark==3.1.3 delta-spark==1.0.0

import os
os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'
os.environ['SPARK_HOME'] = '/usr/local/lib/python3.10/dist-packages/pyspark'

from pyspark.sql import SparkSession
spark = SparkSession.builder \
.appName("DeltaLakeExample") \
.config("spark.jars.packages", "io.delta:delta-core_2.12:1.0.0") \
.config("spark.sql.extensions", "delta.sql.DeltaSparkSessionExtensions") \
.config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
.getOrCreate()







data = [("Alice", 34), ("Bob", 45), ("Cathy", 29)]
columns = ["Name", "Age"]

df = spark.createDataFrame(data, columns)

delta_table_path = "/content/delta_table"

df.write.format("delta").mode("overwrite").save(delta_table_path)

#read contents of data table
df = spark.read.format("delta").load(delta_table_path)
df.show()


new_data = [("David", 30), ("Eva", 22)]
new_df = spark.createDataFrame(new_data, columns)


# Append the new DataFrame to the existing Delta table
new_df.write.format("delta").mode("append").save(delta_table_path)


# Show updated data
updated_df = spark.read.format("delta").load(delta_table_path)
print("Updated Data with New Records:")
updated_df.show()



sampathBoda/netflixHomePage

apt-get install openjdk-11-jdk-headless -qq > /dev/null
!pip install pyspark==3.1.3 delta-spark==1.0.0

import os
from pyspark.sql import SparkSession

os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["PYSPARK_SUBMIT_ARGS"] = "--packages io.delta:delta-core_2.12:1.0.0 pyspark-shell"

spark = SparkSession.builder.appName("DeltaExample").getOrCreate()

data = [("Alice", 34), ("Bob", 45), ("Cathy", 29)]
columns = ["Name", "Age"]
df = spark.createDataFrame(data, columns)
df.show()

delta_table_path = "/content/delta_table"
df.write.format("delta").mode("overwrite").save(delta_table_path)

new_data = [("David", 30), ("Eva", 22)]
df1=spark.createDataFrame(new_data, columns)
df1.write.format("delta").mode("append").save(delta_table_path)

spark.read.format("delta").load(delta_table_path

